{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing  required libraries \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import re, nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix as cfm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping news headlines from india_today using beatifulsoup\n",
    "def get_news(url, no_of_pages):\n",
    "  news = []\n",
    "  for i in range(no_of_pages):\n",
    "    url = url + str(i)\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page)\n",
    "    a_tags = soup.find_all('a')\n",
    "    a_tags_text = [tag.get_text().strip() for tag in a_tags]\n",
    "    sentence_list = [sentence for sentence in a_tags_text if len(sentence)>25]\n",
    "    news = news + sentence_list[:12]\n",
    "  return news\n",
    "\n",
    "number_of_pages=20\n",
    "trending_url = 'https://www.indiatoday.in/trending-news?page='\n",
    "normal_url   = 'https://www.indiatoday.in/world?page='\n",
    "\n",
    "x_trend  = get_news(trending_url,number_of_pages)\n",
    "x_normal = get_news(normal_url,number_of_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Processing\n",
      "\n",
      "['This viral video of a lion cub reminded the Internet of Simba. Watch', 'Bengal woman paints stunning Patachitra to illustrate fight against coronavirus', 'Coronavirus lockdown: Shopping robots deliver free groceries to NHS workers in English town', 'Italians sing Bella Ciao from their balconies to mark 75th Liberation Day', 'Rajasthan HC adjourns matter after lawyer appears in vest during video conference']\n"
     ]
    }
   ],
   "source": [
    "print('Before Processing')\n",
    "print()\n",
    "print(x_trend[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting today's headlines to predict their virality likelihood\n",
    "test_news = get_news('https://www.indiatoday.in/mail-today?page=', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[1]*len(x_trend) + [0]*len(x_normal)\n",
    "train_data=x_trend + x_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning text data through removing punctuation,stopwords & using lemmatisation \n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def data_cleaning(news):\n",
    "    proc=[]\n",
    "    for line in news:\n",
    "        only_letters = re.sub(\"[^a-zA-Z0-9]\", \" \",line) \n",
    "        tokens = nltk.word_tokenize(only_letters)\n",
    "        lower_case = [l.lower() for l in tokens]\n",
    "        filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
    "        lemmas = ' '.join([wordnet_lemmatizer.lemmatize(t) for t in filtered_result])\n",
    "        proc.append(lemmas)\n",
    "    return proc        \n",
    "\n",
    "cleaned_training = data_cleaning(train_data)\n",
    "cleaned_test     = data_cleaning(test_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of data points for training:  480\n",
      "no. of data points predicted:     120\n"
     ]
    }
   ],
   "source": [
    "print('no. of data points for training: ',len(cleaned_training))\n",
    "print('no. of data points predicted:    ',len(cleaned_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using hashing vectorising method for word embedding representation\n",
    "vectorizer = HashingVectorizer(n_features=500)\n",
    "train_vectorized = vectorizer.fit_transform(cleaned_training)\n",
    "test_vectorised  = vectorizer.transform(cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train & validation\n",
    "x_train,x_val,y_train,y_val=train_test_split(train_vectorized,labels,test_size=0.25,shuffle=True,stratify=labels,random_state=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of training data points:   \",x_train.shape[0])\n",
    "print('number of validation data points: ',x_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building and Fitting a simple Logistic Regression Model on training data\n",
    "log_reg=LogisticRegression()\n",
    "log_reg.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results of Logistic Regression model\n",
    "y_pred=log_reg.predict(x_val)\n",
    "print('accuracy: ',log_reg.score(x_val,y_val))\n",
    "print('f1_score: ',f1_score(y_val,y_pred))\n",
    "print('confusion matrix: ')\n",
    "print(cfm(y_val,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the trained regression model for predicting likelihood score on test data\n",
    "prediction=log_reg.predict_proba(test_vectorised)\n",
    "model_predictions=pd.DataFrame({'news':test_news,'virality_likelihood':prediction[:,1]*100})\n",
    "model_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving predictions to csv file\n",
    "model_predictions.to_csv('predicted_virality.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
